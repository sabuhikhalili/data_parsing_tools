{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTICAL TOOLS FOR PARSING DATA WITH PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USED PACKAGES:\n",
    "urllib3                   2.2.1\n",
    "requests                  2.31.0\n",
    "openpyxl                  3.1.2\n",
    "pandas                    2.2.2\n",
    "numpy                     1.26.4\n",
    "beautifulsoup4            4.12.2\n",
    "lxml\n",
    "MechanicalSoup            1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import webbrowser\n",
    "import mechanicalsoup\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Downloading data when a download link is available.\n",
    "This is the ideal case which would often require you not to use the code if you will download the data once from the website. I am still going to show you how to download and read the data using Python if such link is available in case you would have to download the data regularly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***STEP 1: Identify the link***\n",
    "\n",
    "Easiest ways to identify the link:\n",
    "   - a) Go to source website and check for download button. Often download button will be given as image with the image describing the file type. Once you find download button, click right button on your mouse and select \"copy link address\"\n",
    "  ![Example](images/locate_download_button.png)\n",
    "   - b) Sometimes, download button is not directly leading to the main download URL, instead it acts as trigger  - you click on button, it redirects to the original URL. In this case, \"copy link address\" won't give you the correct address. Instead, you may locate downloaded file in the Downloads tab of Chrome browser (CTRL+J will take you there) and you can access correct download link using \"copy link address\" like in this example\n",
    "   ![Example](images/locate_download_link.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***STEP 2. Use the retrieved link to parse data on Python. From the link we can already understand that this is an xlsx file.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://nbg.gov.ge/fm/%E1%83%A1%E1%83%A2%E1%83%90%E1%83%A2%E1%83%98%E1%83%A1%E1%83%A2%E1%83%98%E1%83%99%E1%83%90/monetary_statistics/eng/money-aggregates-and-monetary-ratioseng.xlsx?v=r5z4c\"\n",
    "\n",
    "filename = \"data.xlsx\"\n",
    "\n",
    "# you may first save the file and load it using pandas library. However, saving first as a file may be useful if URL doesn't work and you receive some error info message in text/html form.\n",
    "# By checking response, you may understand what is the issue and try to solve it.\n",
    "response = req.get(URL)\n",
    "\n",
    "# Check if data request was successful\n",
    "if response.ok:\n",
    "    # Save CSV response to a file\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print('Data have been saved successfully ')\n",
    "else:\n",
    "    print('Failed to retrieve data:', response.status_code)\n",
    "\n",
    "df = pd.read_excel(filename, index_col=0, header=2)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://nbg.gov.ge/fm/%E1%83%A1%E1%83%A2%E1%83%90%E1%83%A2%E1%83%98%E1%83%A1%E1%83%A2%E1%83%98%E1%83%99%E1%83%90/monetary_statistics/eng/money-aggregates-and-monetary-ratioseng.xlsx?v=r5z4c\"\n",
    "# Alternatively you can download data using URL directly with pandas\n",
    "df = pd.read_excel(URL, index_col=0, header=2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.loc[df.index.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Sometimes API/URL will let you choose additional options.\n",
    "For example, the following data from Kazakhstan Stock Exchange can be downloaded for specific date range.\n",
    "https://kase.kz/en/money_market/repo-indicators/tonia/archive-xls/22.05.2024/22.05.2024/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from 2010 till current date\n",
    "from_date = \"01.01.2010\"\n",
    "\n",
    "today_date = datetime.today().strftime(\"%d.%m.%Y\")\n",
    "print(today_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://kase.kz/en/money_market/repo-indicators/tonia/archive-xls/\"\n",
    "\n",
    "req_url = f\"{base_url}{from_date}/{today_date}/\"\n",
    "\n",
    "df_2 = pd.read_excel(req_url, index_col=0, header=1)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. In many cases, there won't be a download link, instead data will be given as html table.\n",
    "Downloading such data is not as difficult as it seems. One simply need to locate the table inside html structure by using beautifulsoup package and then put it into desired format. The following data belongs to Bangladesh Interbank market. When users click on the following link https://www.bb.org.bd/en/index.php/monetaryactivity/call_money_market, the webpage with data on interbank market data of latest business day opens up. As a first step, we will only download data for the current day, but I will show you below how the historical data can be downloaded as well. Additionally, you may see that there is no download link available, thus we have to extract data from html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 1: Locate the table first in html structure.***\n",
    "You may make URL request with entire webpage and read the raw response, but it is a tedious approach. Instead we will use Chrome console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press F12 on your keyboard or right click on your mouse and select Inspect. You will see a new pane opening on the right. Click once on inspector tool and then click an area on the table like shown in the picture\n",
    "\n",
    "![Example](images/locate_table_console.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take you to the place where select item is located in entire web html structure. HTML table is designed in hiearchical structure, go either higher or lower in structure until you think that segment covers entire information you need about the table. In most cases, item class will be called table and items in lower hiearchy of the table will have tags as thead and tbody. thead tag refers to the headers of the table, while tbody will contain data. Below is the shown the segment I choose for this example. You may see that whenever I hover my mouse on right pane on an item in html structure, the item is highlighted as well as area it corresponds in the left pane.\n",
    "![Example](images/locate_table_html.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 2: Use the attributes of table to locate it using beautiful soup and later extract the data.***\n",
    "The task here is to help beautiful soup to locate and select part of html which we can see visually on the browser. As you can see, there are two potential characteristics we can use - classname (\"table-responsive\") and tag (\"table\") to locate table. ID would be most ideal, since it would help us uniquely identify table, however, developers of this website have not provided it. Instead we will use classname.In case, there are more than 1 table with such class, we will simply use debugger to see which one is right table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_url = \"https://www.bb.org.bd/en/index.php/monetaryactivity/call_money_market\"\n",
    "response = req.get(req_url)\n",
    "# Check if data request was successful\n",
    "if response.ok:\n",
    "    # Save CSV response to a file\n",
    "    soup = BeautifulSoup(response.content.decode(\"utf-8\"))\n",
    "    print('Data have been saved successfully ')\n",
    "else:\n",
    "    print('Failed to retrieve data:', response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use table tag to locate table\n",
    "tables = soup.find_all(\"table\")\n",
    "print(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use classname attribute\n",
    "# you may use other attributes in this way if they are available\n",
    "tables = soup.find_all(attrs={\"class\": \"table-responsive\"})\n",
    "print(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is another way to find by class\n",
    "tables = soup.find_all(\"div\", class_=\"table-responsive\")\n",
    "print(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first table since we have only 1 of them\n",
    "table = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_raw = table.select_one(\"thead\")\n",
    "t_content = table.select_one(\"tbody\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all <th> elements within <thead>\n",
    "header_cells = headers_raw.find_all(\"th\")\n",
    "\n",
    "# Extract the text from each <th> element\n",
    "headers = [th.get_text(strip=True) for th in header_cells]\n",
    "# this is not the ideal way, but we have to rearrange headers since headers are given in two rows\n",
    "headers.remove(\"Interest rate(%)\")\n",
    "headers.remove(\"Number of Deals\")\n",
    "headers.append(\"Number of Deals\")\n",
    "print(headers)\n",
    "# alternatively you can simply write down column names rather than extract it from html, then make sure the html contains at least your column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all rows in tbody\n",
    "rows = t_content.find_all(\"tr\")\n",
    "\n",
    "# Extract data from each row\n",
    "data = []\n",
    "for row in rows:\n",
    "    cells = row.find_all(\"td\")\n",
    "    if len(cells)==1:\n",
    "        # skip the first row\n",
    "        continue\n",
    "    cell_texts = [cell.get_text(strip=True) for cell in cells]\n",
    "    data.append(cell_texts)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame\n",
    "df_3 = pd.DataFrame(data, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative way to parse html using pandas\n",
    "df_tables = pd.read_html(req_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will create list for each table in html, in our case, there is only one table\n",
    "len(df_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the table\n",
    "df_4 = df_tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas fails to get all the data\n",
    "len(df_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. It is not always straightforward to download data, even using html.\n",
    " In many cases, often using URL alone will not be enough. Website will require you to send additional details in the payload to make it look like an authentic request using browser. Chrome and other browsers do it on the backend, however, if you can open it in the browser, there must be a way to open it using a code as well. Again we will use Chrome console to figure out what is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with above example, now we need to download the data for history as well. We need to define what additional attributes we should send along the URL request to get full data. The process is a bit complicated and not everything can be covered with one example. Often you may require to get creative to identify source. Easiest way is to use Chrome console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 1: Enter Chrome console by using Inspect or pressing F12 and then go to Network.***\n",
    " Then on left pane, select dates to show the full data. In Network, full list of URl requests executed in the backend is listed. Here you need to locate which request is calling the table data. That is the tricky part. Sometimes there are plenty of requests that you may actually spend a lot of time for it. This example is rather easier to find. You may see in the image below that request is called call_money_market and includes link for the website. This is the one we look for.![Example](images/locate_url_request.png)\n",
    "\n",
    "***Step 2: Identify the attributes.***\n",
    " First thing we must identify is how to set dates. There are two places we check. Request headers and payload. Not all the attributes written here are required to get data. Instead we should start with the things we must add and gradually add the rest of the attributes until we get html page with table. In this example, date is set using date_picker attribute in payload. We also send user-agent to identify session as a browser session. You can also see it in request headers. I ended up using following attributes.\n",
    "  ![Example](images/req_attributes.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with local browser to see if you have retrieved the correct html\n",
    "webbrowser.open(\"temp_file.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data\n",
    "tables = soup.find_all(attrs={\"class\": \"table-responsive\"})\n",
    "print(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first table since we have only 1 of them\n",
    "table = tables[0]\n",
    "headers_raw = table.select_one(\"thead\")\n",
    "t_content = table.select_one(\"tbody\")\n",
    "# Find all <th> elements within <thead>\n",
    "header_cells = headers_raw.find_all(\"th\")\n",
    "\n",
    "# Extract the text from each <th> element\n",
    "headers = [th.get_text(strip=True) for th in header_cells]\n",
    "# this is not the ideal way, but we have to rearrange headers since headers are given in two rows\n",
    "headers.remove(\"Interest rate(%)\")\n",
    "headers.remove(\"Number of Deals\")\n",
    "headers.append(\"Number of Deals\")\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all rows in tbody\n",
    "rows = t_content.find_all(\"tr\")\n",
    "\n",
    "# Extract data from each row\n",
    "data = []\n",
    "dates = []\n",
    "current_date = None\n",
    "for row in rows:\n",
    "    cells = row.find_all(\"td\")\n",
    "    if len(cells)==1:\n",
    "        current_date = cells[0].get_text(strip=True)\n",
    "        continue\n",
    "    cell_texts = [cell.get_text(strip=True) for cell in cells]\n",
    "    data.append(cell_texts)\n",
    "    #for each date, there are different types of maturity, thus we need to repeat each date by number of maturity types\n",
    "    dates.append(current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dates to pandas datetime format\n",
    "date_index = pd.to_datetime(dates, format='%d %b, %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = pd.DataFrame(data, columns=headers, index=date_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. There are cases that you have to sign-in in order to access data.\n",
    " If such credentials are required, then it is not possible to simply use URL and get the data. If username and password can be set using attributes, you can use a similiar procedure to the example above and do it. However, another and probably more user-friendly approach is to mimic browser behaviour almost as if you are using your keyboard and mouse to interact with it. In python, mechanicalsoup allows us to fill-in signin forms and make further URL requests in a signed-in type of session. I use Mechanical soup here but has its limitations. Java has selenium packages which is a more advanced tool, however, it is outside scope of this tutorial. Instead I will show far simpler solution in case VI for single-use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are going to sign-in to IMF. We will sign in first and then download the data using Python code. If the data is not in excel format, but an html table, which is when you will mostly need to use the code to parse data, you can stil use the same steps to sign in and do the rest using the examples about parsing from html above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***STEP 1 - Set username and password.***\n",
    " You can write down in the code if you are the only user and you won't share the code. Since I am sharing the code with you, I am going to access it from local file. And later enter login page using mechanical soup browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('user_data.json', 'r') as file:\n",
    "    # Step 3: Load the JSON data\n",
    "    data = json.load(file)\n",
    "\n",
    "# Step 4: Access the attributes\n",
    "username = data['username']\n",
    "password = data['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "req_url = \"https://www.bb.org.bd/en/index.php/monetaryactivity/call_money_market\"\n",
    "# payload = {'date_picker': '01/05/2024 - 31/05/2024'} this format doesn't work\n",
    "\n",
    "data = 'date_picker=01/05/2024 - 31/05/2024'\n",
    "\n",
    "# Define headers with a User-Agent for Chrome\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',\n",
    "    'Content-Type': 'application/x-www-form-urlencoded'\n",
    "}\n",
    "\n",
    "response = req.post(req_url, data=data, headers=headers)\n",
    "\n",
    "# Check if data request was successful\n",
    "if response.ok:\n",
    "    # Save CSV response to a file\n",
    "    soup = BeautifulSoup(response.content.decode(\"utf-8\"))\n",
    "        # Create a temporary file\n",
    "    with open(\"temp_file.html\", 'w', encoding='utf-8') as temp_file:\n",
    "        temp_file.write(response.text)\n",
    "    print('Data have been saved successfully ')\n",
    "else:\n",
    "    print('Failed to retrieve data:', response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 2. Submit your credentials using login form and sign in.***\n",
    " We first need to locate how the form is called inside html structure. Use Chrome console again to locate table in the html and check the tag of form. Luckily they are using default name \"form\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = mechanicalsoup.StatefulBrowser(user_agent='MechanicalSoup')\n",
    "browser.open(\"https://www.bookstore.imf.org/authgatewaylogin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form = browser.select_form('form')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form.set(\"username\", username)\n",
    "form.set(\"password\", password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.submit_selected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.launch_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Finally, in case you have a webpage that requires sign-in and is very complicated or impossible to handle with mechanicalsoup\n",
    " However you still want to parse data from webpage, this is not the ideal but a far simpler solution. Chrome and other browsers allow you to download webpage data. Among the downloaded data, you can find raw html file that includes all the information you need. Instead of making a URL request, we will use that file to extract data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***STEP 1. Click CTRL+S (CMD+S for MAC) or right click on an empty space on the webpage and select Save as.**** Then select folder you want to save. We also need to save complete page. Click save and wait for download to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***STEP 2. Go to the folder with downloaded files.*** You will have one html file with the name you have selected and another folder with the same name. As long as data you want to access is stored in html, you can simply use html file to extract data. Often you may notice that html file itself is 1kb and contains no data, then search html files inside the folder and you will find another html file with bigger size. It may be called resource_content, raw_content etc. Just take the correct html file and nothing else, rename if you want\n",
    "STEP 3. Go to python and use the code below to extract the data. HTML extraction is the same after you load html content to beautifulsoup (the method you use only depends on the content). THe only difference is how to load that content to beautiful soup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Example***: https://www.nbkr.kg/index1.jsp?item=120&lang=ENG\n",
    "Go to this webpage and save html page. The data is in the file, you don't need to check inside folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"source_html_file.html\", encoding=\"utf-8\") as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# extract data\n",
    "tables = soup.find_all(attrs={\"class\": \"table content-table table-striped table-hover table-condensed table-bordered\"})\n",
    "print(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# select the first table since we have only 1 of them\n",
    "table = tables[0]\n",
    "t_content = table.select_one(\"tbody\")\n",
    "tmp = t_content.find_all(\"tr\")\n",
    "headers_raw = tmp[0]\n",
    "# Find all <th> elements within <thead>\n",
    "header_cells = headers_raw.find_all(\"td\")\n",
    "\n",
    "# Extract the text from each <th> element\n",
    "headers = [th.get_text(strip=True) for th in header_cells]\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Find all rows in tbody\n",
    "rows = tmp[1:-1]\n",
    "\n",
    "# Extract data from each row\n",
    "data = []\n",
    "for row in rows:\n",
    "    cells = row.find_all(\"td\")\n",
    "    cell_texts = [cell.get_text(strip=True) for cell in cells]\n",
    "    data.append(cell_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# convert dates to pandas datetime format\n",
    "df_6 = pd.DataFrame(data, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# VI. Parsing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Logic is the same as parsing tables. Just select the location you want to get text and use get_text method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "***Example***: https://www.theguardian.com/business/article/2024/jun/16/recovery-and-interest-rate-cuts-wont-be-enough-to-win-sunak-the-election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# let's get only the news title and text from this webpage\n",
    "# specifically select an area in html and parse text from that area only\n",
    "req_url = \"https://www.theguardian.com/business/article/2024/jun/16/recovery-and-interest-rate-cuts-wont-be-enough-to-win-sunak-the-election\"\n",
    "response = req.get(req_url)\n",
    "\n",
    "# Check if data request was successful\n",
    "if response.ok:\n",
    "    # Save CSV response to a file\n",
    "    soup = BeautifulSoup(response.content.decode(\"utf-8\"))\n",
    "    print('Data have been saved successfully ')\n",
    "else:\n",
    "    print('Failed to retrieve data:', response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "news_text = soup.find(id=\"maincontent\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# let's now try again with more careful approach and remove ads\n",
    "# In inspection, you will see that all the news text under maincontent has tag p\n",
    "texts = soup.find(id=\"maincontent\").find(\"div\", class_=\"article-body-commercial-selector\").find_all(\"p\", recursive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "parsed_text = []\n",
    "for text in texts:\n",
    "    parsed_text.append(text.get_text())\n",
    "news_text_2 = \"\\n\".join(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(news_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1. Extract data from the excel file in the following URL: https://www.bou.or.ug/bou/bouwebsite/bouwebsitecontent/statistics/InterestRates/Interest_rates.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2. Extract data about 1D interbank rates from this website: https://www.bcu.gub.uy/Politica-Economica-y-Mercados/Paginas/Tasa-1-Dia.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BONUS Content: A problem requiring nested solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We need to parse data about lobby groups. However, information is not provided under one table, one has to enter dedicated page for each of them to access full data. Luckily, we can use nested solution to get the data\n",
    "What you type in browser:\n",
    "# https://transparency-register.europa.eu/searchregister-or-update/search-register_en\n",
    "Main URL that actually contains data:\n",
    "P.S. It is tricky to find this URL. When you enter the link above in browser, check the Network in console and you will see many other URL requests, one of them includes the link below. And that link has the table we want. You may see Responses of each URL to decide on the righ one.\n",
    "# https://ec.europa.eu/transparencyregister/public/alphabetical/REGISTRANTS/LATIN/a/1?lang=en\n",
    "Second URL to get info on each lobby group by putting relevant ID for them again what you see on website:\n",
    "# https://transparency-register.europa.eu/search-details_en?id=<lobby_id>\n",
    "Url actually gets data on backend:\n",
    "# https://ec.europa.eu/transparencyregister/public/PUBLIC/ORGANISATION/<lobby_id>?lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get list of IDs\n",
    "page_number = 0 # starts from zero\n",
    "req_url = f\"https://ec.europa.eu/transparencyregister/public/alphabetical/REGISTRANTS/LATIN/a/{page_number}?lang=en\"\n",
    "# Define the minimum required headers\n",
    "response = req.get(req_url)\n",
    "\n",
    "# Check if data request was successful\n",
    "if response.ok:\n",
    "    # Save CSV response to a file\n",
    "    soup = BeautifulSoup(response.content.decode(\"utf-8\"))\n",
    "    print('Data have been saved successfully ')\n",
    "    # Create a temporary file\n",
    "    with open(\"temp_file.html\", 'w', encoding='utf-8') as temp_file:\n",
    "        temp_file.write(response.text)\n",
    "else:\n",
    "    print('Failed to retrieve data:', response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tables = soup.find_all(\"table\", class_=\"ecl-table ecl-table--zebra\")\n",
    "print(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# select the first table since we have only 1 of them\n",
    "table = tables[0]\n",
    "headers_raw = table.select_one(\"thead\")\n",
    "t_content = table.select_one(\"tbody\")\n",
    "# Find all <th> elements within <thead>\n",
    "header_cells = headers_raw.find_all(\"th\")\n",
    "# Extract the text from each <th> element\n",
    "headers = [th.get_text(strip=True) for th in header_cells]\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract data from each row\n",
    "data = []\n",
    "rows = t_content.find_all(\"tr\")\n",
    "for row in rows:\n",
    "    cells = row.find_all(\"td\")\n",
    "    cell_texts = [cell.get_text(strip=True) for cell in cells]\n",
    "    data.append(cell_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lobby_list = pd.DataFrame(data, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lobby_list.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# now let's use IDs to get rest of data\n",
    "id_list = list(lobby_list.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# this is a bulk solution to get all data. If some sub-tables have weird format, and you need those data, you shold treat each table separately,\n",
    "# instead of looping through each table and applying same solution.\n",
    "combined_lobby_data = dict()\n",
    "for _id in id_list:\n",
    "    lob_url = f\"https://ec.europa.eu/transparencyregister/public/PUBLIC/ORGANISATION/{_id}?lang=en\"\n",
    "    response = req.get(lob_url)\n",
    "    soup = BeautifulSoup(response.content.decode(\"utf-8\"))\n",
    "    tables = soup.find_all(\"table\", class_=\"ecl-table ecl-table--zebra\")\n",
    "    data = dict()\n",
    "    for table in tables:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            # second column contains data. we could simply iterate over all tds, however, missing data will have no td tag and thus cause mismatch ibetween header and data\n",
    "            if len(cells)>1:\n",
    "                key = cells[0].get_text(strip=True)\n",
    "                value = cells[1].get_text(strip=True)\n",
    "                data[key] = value\n",
    "    combined_lobby_data[_id] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_7 = pd.DataFrame.from_dict(combined_lobby_data, orient='index').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_7.to_excel(\"lobby_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
